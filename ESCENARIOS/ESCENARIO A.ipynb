{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Escenarios experimentales Internetworking CNN ESCENARIO A\n",
        "Presentado por: Jonathan Toapanta\n",
        "Fecha: 23/02/2023\n"
      ],
      "metadata": {
        "id": "ps31po5TiwJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "REPOSITORIO : https://github.com/jonathan-elian-toapanta/INTERNETWORKING"
      ],
      "metadata": {
        "id": "kPntODjJyGZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPooling1D, BatchNormalization\n",
        "from keras.models import Sequential\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV,train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "BAl8sqTMt9Tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cargar datos y normalizar\n",
        "url = 'https://raw.githubusercontent.com/jonathan-elian-toapanta/INTERNETWORKING/main/ESCENARIOS/ScenarioA.csv'\n",
        "df = pd.read_csv(url, low_memory=False)"
      ],
      "metadata": {
        "id": "1vfSqT4Ot-Ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dfNormalize(df):\n",
        "    for feature_name in df.columns:\n",
        "        df.loc[:,feature_name] = pd.to_numeric(df.loc[:,feature_name], errors='coerce').fillna(0)\n",
        "        max_value = df[feature_name].max()\n",
        "        min_value = df[feature_name].min()\n",
        "        if (max_value - min_value) > 0:\n",
        "            df.loc[:,feature_name] = (df.loc[:,feature_name] - min_value) / (max_value - min_value)\n",
        "        else:\n",
        "            df.loc[:,feature_name] = (df.loc[:,feature_name] - min_value)\n",
        "    return df\n",
        "\n",
        "dataframe = df.reindex(np.random.permutation(df.index)).copy()"
      ],
      "metadata": {
        "id": "dDV19ldBuAl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keys = dataframe.keys()\n",
        "data_to_process = dataframe[keys[4:len(keys)-1]].copy()\n",
        "x_normalised = dfNormalize(data_to_process)"
      ],
      "metadata": {
        "id": "1J_O0BEluMt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = dataframe['label']\n",
        "change_labels = lambda x: 1 if x == 'nonTOR' else 0\n",
        "y_normalised = dataframe['label'].apply(change_labels)\n"
      ],
      "metadata": {
        "id": "WWYUDq5huPNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_normalised, y_normalised, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "6qNVoWh8OqOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the data to fit the input shape of the CNN\n",
        "x_train = x_train.values.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
        "x_test = x_test.values.reshape(x_test.shape[0], x_test.shape[1], 1)\n"
      ],
      "metadata": {
        "id": "V-ZAacnfOqUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the labels to binary format\n",
        "y_train = np.array(y_train).reshape((-1,1))\n",
        "y_test = np.array(y_test).reshape((-1,1))\n"
      ],
      "metadata": {
        "id": "kchyEhERumvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Verificar si hay valores nulos en x_train e y_train\n",
        "print(np.isnan(x_train).any())\n",
        "print(np.isnan(y_train).any())\n",
        "\n",
        "# Reemplazar valores nulos por cero\n",
        "x_train = np.nan_to_num(x_train, nan=0.0)\n",
        "y_train = np.nan_to_num(y_train, nan=0.0)"
      ],
      "metadata": {
        "id": "4T2rR3B2VPTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el modelo\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(x_train.shape[1],1)),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Conv1D(filters=256, kernel_size=3, activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Conv1D(filters=512, kernel_size=3, activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Flatten(),\n",
        "        Dense(units=512, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(units=1, activation='sigmoid')\n",
        "        ])\n",
        "    # Compilar el modelo\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    return model\n"
      ],
      "metadata": {
        "id": "qNkuzm0_O1GR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the KerasClassifier\n",
        "model = KerasClassifier(build_fn=create_model, verbose=1)\n"
      ],
      "metadata": {
        "id": "wY75CU9ntjGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the grid search parameters\n",
        "batch_size = [32, 64, 128]\n",
        "epochs = [10, 15, 20]\n",
        "param_grid = dict(batch_size=batch_size, epochs=epochs)\n"
      ],
      "metadata": {
        "id": "jcBKtzvh_M1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the grid search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(x_train, y_train)\n"
      ],
      "metadata": {
        "id": "BrL5DzmOPJhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the results\n",
        "print(\"Mejor: %f usando %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "print(\"%f (%f) con: %r\" % (mean, stdev, param))"
      ],
      "metadata": {
        "id": "ROsD5XlZPKoC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}